# Hyperparameter grid for TF-IDF + Logistic Regression
# This config supports pipeline-level grid search across both vectorizer and classifier

# TfidfVectorizer parameters:
# max_df: Maximum document frequency (ignore common words).
#   - Float: Proportion (e.g., 0.9 = ignore words in >90% of docs).
vectorizer__max_df: [0.8, 0.9, 0.95]

# min_df: Minimum document frequency (ignore rare words).
#   - Integer: Absolute count.
vectorizer__min_df: [2, 3, 5]

# ngram_range: Range of n-grams to extract.
#   - (1, 1): Only unigrams.
#   - (1, 2): Unigrams + bigrams (better for sentiment like "not good").
#   - (1, 3): Adds trigrams.
vectorizer__ngram_range: [[1, 1], [1, 2]]

# max_features: Maximum vocabulary size.
vectorizer__max_features: [10000, 15000, null]

# sublinear_tf: Apply sublinear tf scaling (replace tf with 1 + log(tf)).
#   - Reduces impact of very frequent terms.
vectorizer__sublinear_tf: [true, false]

# Logistic Regression parameters:
# C: Inverse of regularization strength.
#   - Lower C (e.g., 0.01): Stronger regularization, simpler model.
#   - Higher C (e.g., 100): Weaker regularization, more complex model.
classifier__C: [0.01, 0.1, 1.0, 10.0, 100.0]

# max_iter: Maximum iterations for convergence.
#   - Increase if seeing convergence warnings.
classifier__max_iter: [1000, 2000, 3000]

# penalty: Regularization type.
#   - 'l2': Ridge (default, handles multicollinearity well).
#   - 'l1': Lasso (feature selection, sparse solutions).
classifier__penalty: ['l1', 'l2']

# solver: Optimization algorithm.
#   - 'lbfgs': Good for small datasets, only supports l2.
#   - 'liblinear': Good for small datasets, supports l1 and l2.
#   - 'saga': Supports all penalties, good for large datasets.
classifier__solver: ['liblinear', 'saga']

# Cross-validation folds (applies to GridSearchCV)
cv_folds: 5